# Spark stat analyzer

POC to generate consolidated statistics from json files (generated by navitia-stat-logger or navitia-stat-exporter)

## Pre-requisites

* Spark 1.5+ (may work with previous versions, but untested)
* A repository with exported statistics files where stat files are stored in a tree like

```
  |
  \- <year>
      |
      \- <month>
          |
          \- <day>
              |
              \- <files>.json.log(.gz)
```

The files are json logs (one json per line). The files may be compressed using gzip.

Create config.py file from template config.py.dist and adapt them

## Usage

```
SPARK_BIN=/path/to/spark/bin/ MASTER_OPTION=local[3] TABLE=<your_table> CONTAINER_STORE_PATH=/absolute/path/to/data START_DATE=<start_date> END_DATE=<end_date> ./run_analyzer.sh
```

where:
* your_table: analyzer/table name, possible value : see includes/utils.py:9

* start_date and end_date is in YYYY-MM-DD format

### Shell ZSH users

Launching 'spark-submit' command , if you get error :

    zsh: no matches found

You can replace in command line `--master='local[3]'` by `--master=local`, or use *bash*.

## Tests

To run tests, you can build your own image and launch them using:

```
docker-compose -f docker-composer.test.yml build
docker-compose -f docker-composer.test.yml run spark-stat-analyser
docker-compose -f docker-composer.test.yml down --remove-orphans
```

The docker image is also hosted on our registry under the same name:tag

License
-------

This bundle is released under the [GPL-3.0 License](LICENSE)